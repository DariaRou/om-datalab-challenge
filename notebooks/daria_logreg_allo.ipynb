{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97053d3-6332-49db-a022-1ce119efd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894bd512-06c6-4964-bc6f-51a88d3c4daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dariarousset/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dariarousset/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dariarousset/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bff7c217-8468-4f28-9276-3956616c7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6910bcad-6fe5-4a03-9bc4-60da013b24b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daria_logreg_frenchtweets.ipynb',\n",
       " 'ml_model_nb_allo.pkl',\n",
       " 'xav_Deep_Learning_CNN.ipynb',\n",
       " 'xav_Machine_Learning.ipynb',\n",
       " 'Deep_Learning_CNN.ipynb',\n",
       " 'twitter_api.ipynb',\n",
       " 'processing(fr).ipynb',\n",
       " 'xav_Deep_Learning.ipynb',\n",
       " 'data_preprocessing-2.ipynb',\n",
       " '.keep',\n",
       " 'daria_nb_frenchtweets.ipynb',\n",
       " 'RNN-2.ipynb',\n",
       " 'daria_bert.ipynb',\n",
       " 'daria_logreg_frenchtweets_copy.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'daria_nb_allocine.ipynb',\n",
       " 'ml_model_nb.pkl',\n",
       " 'daria_RNN.ipynb',\n",
       " 'xav_Preprocessing twitter.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../raw_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9dfb582-e065-466e-9c55-15329dcbad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5029\n",
       "1    4971\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french = pd.read_csv('../raw_data/allo_clean.csv').sample(10000).reset_index(drop=True)\n",
    "french.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a05d76-7438-4a17-b7ec-ac793fda73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= french['clean_text']\n",
    "y= french['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b25822-01f3-4236-8f2a-3a9d7f3361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 240 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words={'ai', 'aie',\n",
       "                                                                    'aient',\n",
       "                                                                    'aies',\n",
       "                                                                    'ait', 'as',\n",
       "                                                                    'au',\n",
       "                                                                    'aura',\n",
       "                                                                    'aurai',\n",
       "                                                                    'auraient',\n",
       "                                                                    'aurais',\n",
       "                                                                    'aurait',\n",
       "                                                                    'auras',\n",
       "                                                                    'aurez',\n",
       "                                                                    'auriez',\n",
       "                                                                    'aurions',\n",
       "                                                                    'aurons',\n",
       "                                                                    'auront',\n",
       "                                                                    'aux',\n",
       "                                                                    'avaient',\n",
       "                                                                    'avais',\n",
       "                                                                    'avait',\n",
       "                                                                    'avec',\n",
       "                                                                    'avez',\n",
       "                                                                    'aviez',\n",
       "                                                                    'avions',\n",
       "                                                                    'avons',\n",
       "                                                                    'ayant',\n",
       "                                                                    'ayante',\n",
       "                                                                    'ayantes', ...})),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegressio...\n",
       "                                                           verbose=1))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__C': array([1.00000000e-04, 7.74263683e-04, 5.99484250e-03, 4.64158883e-02,\n",
       "       3.59381366e-01, 2.78255940e+00, 2.15443469e+01, 1.66810054e+02,\n",
       "       1.29154967e+03, 1.00000000e+04]),\n",
       "                         'tfidf__lowercase': (True, False),\n",
       "                         'tfidf__max_df': [0.6, 0.65, 0.7, 0.75, 0.85, 1],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2)]},\n",
       "             return_train_score=True, scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\n",
    "     'tfidf__lowercase': (True, False),\n",
    "     'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__max_df': [0.60, 0.65, 0.70, 0.75, 0.85, 1],\n",
    "     'clf__C': np.logspace(-4, 4, 10),\n",
    "}\n",
    "\n",
    "tfidf_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop)),\n",
    "    ('clf', LogisticRegression(penalty='l2', n_jobs=-1, verbose=1)),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    tfidf_clf, param_grid, cv=10, \n",
    "    scoring='f1', return_train_score=True, \n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "325e604a-5383-4a4e-8dd0-1af2f251b972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10000.0,\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 0.6,\n",
       " 'tfidf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbee487-d1fb-4789-ba5c-e1af67f063a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7654843272647883"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853575d-7e7f-44f0-9981-5027bec529c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43873820-191f-4c75-85b5-c24b901bc068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db067a4f-1cff-49a1-8cc5-6ea21aaacd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dba1a4-9537-4a5b-ac45-c1b061f2e434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c78b2-57c6-450f-81d7-45678312c208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992b998-5226-4275-9379-81fcf2e6619e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a0829-b78f-480c-869e-e688b24b8827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "439ee3a6-8995-4c27-afe6-6d6cf5661d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.read_csv(\"tweet_psg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580ac468-6f28-4b81-a85d-17e0ff97cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import spacy\n",
    "stop = set(stopwords.words('french'))\n",
    "nlp=spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    #emojis=demoji.findall(text)\n",
    "    #if emojis != {}:\n",
    "        #for key,value in emojis.items(): \n",
    "            #if key in text:\n",
    "                #try:\n",
    "                    #translated_text=ts.translate_html(value, translator=ts.google, to_language='fr', n_jobs=-1)\n",
    "                    #text=text.replace(key,translated_text)\n",
    "                #except TypeError:\n",
    "                    #pass\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # remove puncutation\n",
    "    for punctuation in string.punctuation.replace('#',''):\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    # remove words that contain numbers\n",
    "    text = ''.join(letter for letter in text if not letter.isdigit())\n",
    "    #tokenization + remove stop words\n",
    "    doc=nlp(text)\n",
    "    lemmatized= [token.lemma_ for token in doc]\n",
    "    # join all\n",
    "    text = \" \".join(lemmatized)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9df8bdc7-869c-4b1c-b37a-f2d0e479bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[\"clean_text\"] = predict[\"tweet\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b6fa31-936e-4899-9634-184d1d39322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = predict['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ca14956-f632-4a55-9c07-d75ec061d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(grid_search.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce0ca187-c79e-49b2-a047-be9940c75e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_twits_prediction = pd.concat([predict,prediction], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8472483f-3182-4386-829b-47ad771df5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_twits_prediction.to_csv(\"prediction_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0b17f-32be-49a0-9f4c-d0429823a522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
